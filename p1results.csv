Method,Epoch Num,Top 1 Training,Top 5 Training,% Training,Top 1 Validation,Top 5 Validation,% Validation,,
Dropout (0.1),1,2692,10841,0.1084,264,1052,0.1052,,100000
Dropout (0.1),2,3700,14165,0.1417,348,1393,0.1393,,10000
Dropout (0.1),3,4857,16851,0.1685,451,1680,0.1680,,
Dropout (0.2),1,2669,10223,0.1022,237,989,0.0989,,
Dropout (0.2),2,3540,12315,0.1232,337,1201,0.1201,,
Dropout (0.2),3,4268,15470,0.1547,416,1526,0.1526,,
Base,1,3143,12154,0.1215,312,1217,0.1217,,
Base,2,4034,16384,0.1638,394,1617,0.1617,,
Base,3,5733,19899,0.1990,531,2024,0.2024,,
LR (0.01),1,8973,27684,0.2768,888,2719,0.2719,,
LR (0.01),2,14329,37494,0.3749,1416,3734,0.3734,,
LR (0.01),3,18015,44616,0.4462,1749,4415,0.4415,,
LR (0.0001),1,1136,5679,0.0568,112,581,0.0581,,
LR (0.0001),2,1536,6717,0.0672,145,671,0.0671,,
LR (0.0001),3,2017,8296,0.0830,192,835,0.0835,,
LR/WD (0.01),1,9690,29461,0.2946,954,2953,0.2953,,
LR/WD (0.01),2,13450,36446,0.3645,1324,3643,0.3643,,
LR/WD (0.01),3,14206,37857,0.3786,1456,3773,0.3773,,
Adagrad,1,20524,47962,0.4796,1954,4744,0.4744,,
Adagrad,2,25050,54600,0.5460,2390,5378,0.5378,,
Adagrad,3,28304,58897,0.5890,2658,5640,0.5640,,
Adam,1,22333,51927,0.5193,2196,5090,0.5090,,
Adam,2,33081,64867,0.6487,3164,6357,0.6357,,
Adam,3,39923,71741,0.7174,3710,6773,0.6773,,